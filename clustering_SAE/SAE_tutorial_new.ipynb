{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0. Preparing packages and modules**"
      ],
      "metadata": {
        "id": "QU-tOvn-BGqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phenograph\n",
        "!pip install umap-learn\n",
        "!pip install scanpy\n",
        "!pip install louvain\n",
        "!pip install git+https://github.com/saketkc/pysctransform.git"
      ],
      "metadata": {
        "id": "Q8wb6RIGBOze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from tensorflow.keras.losses import MSE, KLD\n",
        "import phenograph\n",
        "\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from pysctransform import vst, get_hvg_residuals, SCTransform\n",
        "import anndata as ad\n",
        "import scanpy as sc\n",
        "from scipy.sparse import *"
      ],
      "metadata": {
        "id": "zVNMgxz0BZjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3yzaw4cvEa5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1. Preprocessing**\n",
        "\n",
        "Here we use Scanpy and pySCTransform to normalize data.\n",
        "\n",
        "You can use SCTransform in the Seurat R package alternatively."
      ],
      "metadata": {
        "id": "ZkWIs3ST_5dl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v17MM8e0_lwr"
      },
      "outputs": [],
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/pbmc_idents.csv\", index_col=0, sep=\"\\t\"))\n",
        "# For datasets with no labels,\n",
        "# idents = np.zeros(n_sample)\n",
        "df = pd.read_csv(\"/content/drive/My Drive/compbio/test_data/pbmc_raw.csv\", index_col=0, sep=\"\\t\")\n",
        "count = np.array(df)\n",
        "# row: cells\n",
        "# columan: genes\n",
        "# or directly upload SCTransformed data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata=sc.AnnData(count)"
      ],
      "metadata": {
        "id": "ep65lfM6iAQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cells = df.index.values.astype('str')\n",
        "#genes = df.columns.values.astype('str')\n",
        "#adata.var_names = genes"
      ],
      "metadata": {
        "id": "lAz0QCJQibnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "adata.X = csr_matrix(adata.X)\n",
        "residuals = SCTransform(adata, var_features_n=3000)"
      ],
      "metadata": {
        "id": "R1hq1OOgin87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2. Building model**"
      ],
      "metadata": {
        "id": "dSaRk-kAAKHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/compbio/gitclone/clustering_SAE\")\n",
        "from utils import *\n",
        "from layers import DenseTranspose\n",
        "from sae import SAE"
      ],
      "metadata": {
        "id": "ZLX5IRKHAVDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(residuals)\n",
        "idents = idents.flatten()\n",
        "[n_sample, n_gene] = count.shape\n",
        "x_train = count.astype('float32')\n",
        "idents = idents.astype('str')\n",
        "idents_new = id2number(idents)"
      ],
      "metadata": {
        "id": "6KEo6L5VjCzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = SAE(x_train, idents, n_sample, n_gene, n_sample)"
      ],
      "metadata": {
        "id": "iTreogPHjfxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3. Pre-training**"
      ],
      "metadata": {
        "id": "b0uaHFkTA5Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = autoencoder.train1()\n",
        "h = autoencoder.train2(h)\n",
        "h = autoencoder.train3(h)\n",
        "h = autoencoder.train4(h)\n",
        "autoencoder.train(max_epoch=100)"
      ],
      "metadata": {
        "id": "uC9ApFpWA-Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-SNE\n",
        "encoded_data = autoencoder.ec(x_train)\n",
        "ed = np.array(encoded_data)\n",
        "ed = dotsne(ed)\n",
        "myscatter(ed, idents, legend=True)"
      ],
      "metadata": {
        "id": "xamao3_SjuyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UMAP\n",
        "encoded_data = autoencoder.ec(x_train)\n",
        "embeddings = np.array(encoded_data)\n",
        "embeddings = doumap(ed)\n",
        "myscatter(embeddings, idents, legend=True)"
      ],
      "metadata": {
        "id": "YMH5mPapjziv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "labels, _,  _ = phenograph.cluster(np.array(encoded_data))\n",
        "measure(idents, labels)"
      ],
      "metadata": {
        "id": "HARZFk-tkDx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4. Clustering training**"
      ],
      "metadata": {
        "id": "nrHbuxjaA9in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.clustering_train(max_epoch=50)"
      ],
      "metadata": {
        "id": "NiFAYIZpBCAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-SNE\n",
        "encoded_data = autoencoder.ec(x_train)\n",
        "ed = np.array(encoded_data)\n",
        "ed = dotsne(ed)\n",
        "myscatter(ed, idents, legend=True)"
      ],
      "metadata": {
        "id": "3KRATg8lkBEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UMAP\n",
        "encoded_data = autoencoder.ec(x_train)\n",
        "embeddings = np.array(encoded_data)\n",
        "embeddings = doumap(ed)\n",
        "myscatter(embeddings, idents, legend=True)"
      ],
      "metadata": {
        "id": "MoD8YHrskDJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "labels, _,  _ = phenograph.cluster(np.array(encoded_data))\n",
        "measure(idents, labels)"
      ],
      "metadata": {
        "id": "2AANLVa4kbd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5. Saving results**"
      ],
      "metadata": {
        "id": "9Ofgx2czk4AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.autoencoder.save_weights('autoencoder_pbmc.h5')"
      ],
      "metadata": {
        "id": "fPdeYHEoke0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = pd.DataFrame(np.array(encoded_data))\n",
        "save.to_csv('/content/drive/My Drive/compbio/test_data/pbmc_model.csv',index=False,header=True)"
      ],
      "metadata": {
        "id": "eKvT8JRGkg3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = pd.DataFrame(ed)\n",
        "save.to_csv('/content/drive/My Drive/compbio/test_data/pbmc_model_tsne.csv',index=False,header=True)"
      ],
      "metadata": {
        "id": "vQ21i-LzkpL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = pd.DataFrame(embeddings)\n",
        "save.to_csv('/content/drive/My Drive/compbio/test_data/pbmc_model_umap.csv',index=False,header=True)"
      ],
      "metadata": {
        "id": "n4r7s3Zokzms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}