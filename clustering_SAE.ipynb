{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install phenograph"
      ],
      "metadata": {
        "id": "r2Z4YzQXlMeb"
      },
      "id": "r2Z4YzQXlMeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V6f39ED9nmNf"
      },
      "id": "V6f39ED9nmNf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9bf651",
      "metadata": {
        "id": "4f9bf651"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from tensorflow.keras.losses import MSE, KLD\n",
        "import phenograph\n",
        "\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no\n",
        "tf.random.set_seed(123)"
      ],
      "metadata": {
        "id": "2ZcVN3I8MBSw"
      },
      "id": "2ZcVN3I8MBSw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac04ef15",
      "metadata": {
        "id": "ac04ef15"
      },
      "outputs": [],
      "source": [
        "def myscatter(Y, class_idxs, legend=False, ran=True, seed=229):\n",
        "    if ran:\n",
        "        np.random.seed(seed)\n",
        "    Y = np.array(Y)\n",
        "    fig, ax = plt.subplots(figsize=(5,4), dpi=300)\n",
        "    classes = list(np.unique(class_idxs))\n",
        "    markers = 'osD' * len(classes)\n",
        "    colors = plt.cm.rainbow(np.linspace(0, 1, len(classes)))\n",
        "    if ran:\n",
        "        np.random.shuffle(colors)\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        mark = markers[i]\n",
        "        ax.plot(Y[class_idxs == cls, 0], Y[class_idxs == cls, 1], marker=mark,\n",
        "                linestyle='', ms=4, label=str(cls), alpha=1, color=colors[i],\n",
        "                markeredgecolor='black', markeredgewidth=0.15)\n",
        "    if legend:\n",
        "        ax.legend(bbox_to_anchor=(1.03, 1), loc=2, borderaxespad=0, fontsize=10, markerscale=2, frameon=False,\n",
        "                  ncol=2, handletextpad=0.1, columnspacing=0.5)\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    return ax\n",
        "\n",
        "def dotsne(X, dim=2, ran=23):\n",
        "    tsne = TSNE(n_components=dim, random_state=ran)\n",
        "    Y_tsne = tsne.fit_transform(X)\n",
        "    return Y_tsne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/idents1_raw_1.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/data1_raw_1_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "WkXBREZ1sg8i"
      },
      "id": "WkXBREZ1sg8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/idents3_raw_1.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/data3_raw_1_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "YsF_L4lcruRl"
      },
      "id": "YsF_L4lcruRl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/ileum_cell_info.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/ileum_raw_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "OCPqRifksBrH"
      },
      "id": "OCPqRifksBrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/colon_cell_info.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/colon_raw_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "7Oow07X1t4xt"
      },
      "id": "7Oow07X1t4xt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/pbmc_idents.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/pbmc_raw_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "3hMebs3lw3zV"
      },
      "id": "3hMebs3lw3zV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/bm4/bm4_idents.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/bm4/bm4_raw_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "e0Q4U6EcuU1D"
      },
      "id": "e0Q4U6EcuU1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/mousebrain/mousebrain_idents.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/mousebrain/mousebrain_raw_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "OSHKFFaZyjJy"
      },
      "id": "OSHKFFaZyjJy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/IPMN_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "CnNYuFkkRJ_l"
      },
      "id": "CnNYuFkkRJ_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/compbio/test_data/\")"
      ],
      "metadata": {
        "id": "4F_9yVyZZ9Wt"
      },
      "id": "4F_9yVyZZ9Wt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.load(\"pbmc_pysct.npy\")\n",
        "idents = np.load(\"pbmc_idents.npy\")"
      ],
      "metadata": {
        "id": "8BxPE8CRYKUT"
      },
      "id": "8BxPE8CRYKUT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.load(\"ileum_norm.npy\")\n",
        "idents = np.load(\"ileum_idents.npy\")"
      ],
      "metadata": {
        "id": "gydTjjkCZ_Ky"
      },
      "id": "gydTjjkCZ_Ky",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.load(\"IPMN_pysct.npy\")"
      ],
      "metadata": {
        "id": "radmxxXVur7Q"
      },
      "id": "radmxxXVur7Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988e058d",
      "metadata": {
        "id": "988e058d"
      },
      "outputs": [],
      "source": [
        "count = np.transpose(count)\n",
        "idents = idents.flatten()\n",
        "[n_sample, n_gene] = count.shape\n",
        "x_train = count.astype('float32')\n",
        "idents = idents.astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9136fdb",
      "metadata": {
        "id": "d9136fdb"
      },
      "outputs": [],
      "source": [
        "count = np.transpose(count)\n",
        "[n_sample, n_gene] = count.shape\n",
        "x_train = count.astype('float32')\n",
        "idents = np.zeros(n_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3e3dae",
      "metadata": {
        "id": "1a3e3dae"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import SpectralClustering\n",
        "def clustering(h, n_cluster, k=15, f=\"kmeans\"):\n",
        "    if f == \"kmeans\":\n",
        "        labels = KMeans(n_clusters=n_cluster, random_state=0).fit(h).labels_\n",
        "    elif f == \"spectral\":\n",
        "        labels = SpectralClustering(n_clusters=n_cluster, affinity=\"precomputed\", assign_labels=\"discretize\",\n",
        "                                    random_state=0).fit_predict(adj)\n",
        "    \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc809b2c",
      "metadata": {
        "id": "cc809b2c"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Layer\n",
        "class DenseTranspose(Layer):\n",
        "    def __init__(self, dense, activation=None, **kwargs):\n",
        "        self.dense = dense\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        super().__init__(**kwargs)\n",
        "    def build(self, batch_input_shape):\n",
        "        self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\",\n",
        "                                      shape=[self.dense.input_shape[-1]])\n",
        "        super().build(batch_input_shape)\n",
        "    def call(self, inputs):\n",
        "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
        "        return self.activation(z + self.biases)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def _nan2zero(x):\n",
        "    return tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
        "\n",
        "def _nan2inf(x):\n",
        "    return tf.where(tf.math.is_nan(x), tf.zeros_like(x)+np.inf, x)\n",
        "\n",
        "def cal_zinb(pi=x_train, disp=x_train, ridge_lambda=0.0, x_pred0=x_train, x_true0=x_train):\n",
        "    x_true = tf.cast(x_true0, tf.float32)\n",
        "    x_pred = tf.cast(x_pred0, tf.float32)\n",
        "    \n",
        "    eps = 1e-10\n",
        "    theta = tf.minimum(disp, 1e6)\n",
        "    t1 = tf.math.lgamma(theta+eps) + tf.math.lgamma(x_true+1.0) - tf.math.lgamma(x_true+theta+eps)\n",
        "    t2 = (theta+x_true) * tf.math.log(1.0 + (x_pred/(theta+eps))) + (x_true * (tf.math.log(theta+eps) - tf.math.log(x_pred+eps)))\n",
        "    final = t1 + t2\n",
        "    final = _nan2inf(final)\n",
        "\n",
        "    nb_case = final - tf.math.log(1.0-pi+eps)\n",
        "    zero_nb = tf.math.pow(theta/(theta+x_pred+eps), theta)\n",
        "    zero_case = -tf.math.log(pi + ((1.0-pi)*zero_nb)+eps)\n",
        "    result = tf.where(tf.less(x_true, 1e-8), zero_case, nb_case)\n",
        "    ridge = ridge_lambda*tf.square(pi)\n",
        "    result += ridge\n",
        "\n",
        "    result = tf.reduce_mean(result)\n",
        "\n",
        "    result = _nan2inf(result)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "2Js0HM3-67A_"
      },
      "id": "2Js0HM3-67A_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f414b22b",
      "metadata": {
        "id": "f414b22b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import activations, constraints, initializers, regularizers\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class ClusteringLayer(Layer):\n",
        "    def __init__(self, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=None, initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def target_distribution(q):\n",
        "    q = q.numpy()\n",
        "    weight = q ** 2 / q.sum(0)\n",
        "    return (weight.T / weight.sum(1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721433ee",
      "metadata": {
        "id": "721433ee"
      },
      "outputs": [],
      "source": [
        "# no\n",
        "clustering_layer = ClusteringLayer(name='clustering', input_shape=(25,))\n",
        "clustering = keras.models.Sequential([clustering_layer])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_cluster(hidden, clusters, alpha):\n",
        "    clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
        "    q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(hidden, axis=1) - clusters), axis=2) / alpha))\n",
        "    q **= (alpha + 1.0) / 2.0\n",
        "    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "    return q\n",
        "\n",
        "def cal_dist(hidden, clusters):\n",
        "    clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
        "    dist1 = K.sum(K.square(K.expand_dims(hidden, axis=1) - clusters), axis=2)\n",
        "    temp_dist1 = dist1 - tf.reshape(tf.reduce_min(dist1, axis=1), [-1, 1])\n",
        "    temp_dist1 = K.transpose(K.transpose(temp_dist1) / K.max(temp_dist1, axis=1))\n",
        "    q = K.exp(-temp_dist1 * 10)\n",
        "    #q = K.exp(-temp_dist1)\n",
        "    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "    q = K.pow(q, 2)\n",
        "    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "    dist2 = dist1 * q\n",
        "    return dist1, dist2\n",
        "\n",
        "def cal_latent(hidden, alpha):\n",
        "    sum_y = K.sum(K.square(hidden), axis=1)\n",
        "    num = -2.0 * tf.matmul(hidden, tf.transpose(hidden)) + tf.reshape(sum_y, [-1, 1]) + sum_y\n",
        "    num = num / alpha\n",
        "    num = tf.pow(1.0 + num, -(alpha + 1.0) / 2.0)\n",
        "    zerodiag_num = num - tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "    latent_p = K.transpose(K.transpose(zerodiag_num) / K.sum(zerodiag_num, axis=1))\n",
        "    return num, latent_p\n",
        "\n",
        "def target_dis(latent_p):\n",
        "    latent_q = tf.transpose(tf.transpose(tf.pow(latent_p, 2)) / tf.reduce_sum(latent_p, axis = 1))\n",
        "    return tf.transpose(tf.transpose(latent_q) / tf.reduce_sum(latent_q, axis = 1))\n",
        "\n",
        "def computeCentroids(data, labels):\n",
        "    n_clusters = len(np.unique(labels))\n",
        "    data_1 = np.array(data)\n",
        "    return np.array([data_1[labels == i].mean(0) for i in range(n_clusters)])\n",
        "\n",
        "def get_centers(Y):\n",
        "    #from sklearn.cluster import SpectralClustering\n",
        "    l, _,  _ = phenograph.cluster(Y)\n",
        "    #l = KMeans(n_clusters=10, random_state=0).fit(Y).labels_\n",
        "    centers = computeCentroids(Y, l)\n",
        "    return centers, l\n",
        "\n",
        "def get_centers_kmeans(Y, k):\n",
        "    #from sklearn.cluster import SpectralClustering\n",
        "    #l, _,  _ = phenograph.cluster(Y)\n",
        "    l = KMeans(n_clusters=k, random_state=0).fit(Y).labels_\n",
        "    centers = computeCentroids(Y, l)\n",
        "    return centers, l"
      ],
      "metadata": {
        "id": "FMLYPrl587D7"
      },
      "id": "FMLYPrl587D7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def make_matrix(a, b, n):\n",
        "    mat = np.zeros([n, n])\n",
        "    x = np.zeros(n)\n",
        "    y = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        x[i] = a[a==i].size\n",
        "        y[i] = b[b==i].size\n",
        "    #print(x)\n",
        "    #print(y)\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            x0 = x[i]\n",
        "            y0 = y[j]\n",
        "            mat[i,j] = 1 - min(x0,y0) / max(x0, y0)\n",
        "    return mat\n",
        "\n",
        "def update_labels(l, mat):\n",
        "    x,y = linear_sum_assignment(mat)\n",
        "    n = len(l)\n",
        "    dic = dict(zip(x,y))\n",
        "    #print(dic)\n",
        "    for i in range(n):\n",
        "        l[i] = dic[l[i]]\n",
        "    return l"
      ],
      "metadata": {
        "id": "lwASTLuLKUSj"
      },
      "id": "lwASTLuLKUSj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27637baf",
      "metadata": {
        "id": "27637baf"
      },
      "outputs": [],
      "source": [
        "class AE(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, encoder, decoder, zinb_model):\n",
        "        super(AE, self).__init__()\n",
        "        self.ec = encoder\n",
        "        self.dc = decoder\n",
        "        #self.clustering = clustering\n",
        "        self.nb = zinb_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d838ea",
      "metadata": {
        "id": "74d838ea"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import MSE, KLD\n",
        "import math\n",
        "from keras.layers import Lambda\n",
        "\n",
        "\n",
        "class SAE():\n",
        "\n",
        "    def __init__(self, X):\n",
        "        super(SAE, self).__init__()\n",
        "        encoded1 = Dense(512, activation = \"relu\", kernel_constraint = keras.constraints.NonNeg(), input_shape=(n_gene,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "        encoded2 = Dense(256, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(512,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "        encoded3 = Dense(128, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(256,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "        encoded4 = Dense(25, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(128,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "\n",
        "        encoder = keras.models.Sequential([encoded1, encoded2, encoded3, encoded4])\n",
        "        encoder1 = keras.models.Sequential([encoded1])\n",
        "        encoder2 = keras.models.Sequential([encoded2])\n",
        "        encoder3 = keras.models.Sequential([encoded3])\n",
        "        encoder4 = keras.models.Sequential([encoded4])\n",
        "\n",
        "        decoded1 = DenseTranspose(encoded4, activation = \"relu\")\n",
        "        decoded2 = DenseTranspose(encoded3, activation = \"relu\")\n",
        "        decoded3 = DenseTranspose(encoded2, activation = \"relu\")\n",
        "        decoded4 = DenseTranspose(encoded1, activation = \"relu\")\n",
        "\n",
        "        decoder = keras.models.Sequential([decoded1, decoded2, decoded3, decoded4])\n",
        "        decoder1 = keras.models.Sequential([decoded1])\n",
        "        decoder2 = keras.models.Sequential([decoded2])\n",
        "        decoder3 = keras.models.Sequential([decoded3])\n",
        "        decoder4 = keras.models.Sequential([decoded4])\n",
        "\n",
        "        autoencoder1 = keras.models.Sequential([encoded1, decoded4])\n",
        "        autoencoder2 = keras.models.Sequential([encoded2, decoded3])\n",
        "        autoencoder3 = keras.models.Sequential([encoded3, decoded2])\n",
        "        autoencoder4 = keras.models.Sequential([encoded4, decoded1])\n",
        "\n",
        "        self.X = X\n",
        "        self.ec1 = encoder1\n",
        "        self.ec2 = encoder2\n",
        "        self.ec3 = encoder3\n",
        "        self.ec4 = encoder4\n",
        "        self.dc1 = decoder1\n",
        "        self.dc2 = decoder2\n",
        "        self.dc3 = decoder3\n",
        "        self.dc4 = decoder4\n",
        "        self.autoencoder1 = autoencoder1\n",
        "        self.autoencoder2 = autoencoder2\n",
        "        self.autoencoder3 = autoencoder3\n",
        "        self.autoencoder4 = autoencoder4\n",
        "        self.ec = encoder\n",
        "        self.dc = decoder\n",
        "        self.autoencoder = keras.models.Sequential([encoder, decoder])\n",
        "        self.clustering = clustering\n",
        "\n",
        "\n",
        "\n",
        "        MeanAct = lambda x: tf.clip_by_value(K.exp(x), 1e-5, 1e6)\n",
        "        DispAct = lambda x: tf.clip_by_value(tf.nn.softplus(x), 1e-4, 1e4)\n",
        "\n",
        "        y = Input(shape=(512,))\n",
        "        pi = Dense(n_gene, activation='sigmoid', name='pi')(y)\n",
        "        disp = Dense(n_gene, activation=DispAct, name='dispersion')(y)\n",
        "        mean = Dense(n_gene, activation=MeanAct, name='mean')(y)\n",
        "\n",
        "        zinb_model = keras.models.Model(inputs=[y], outputs=[pi, disp, mean])\n",
        "\n",
        "        self.AE = AE(encoder, decoder, zinb_model)\n",
        "        self.nb = zinb_model\n",
        "        \n",
        "        \n",
        "    def train1(self, max_epoch=100, lr=0.002):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr)\n",
        "        for epoch in range(0, max_epoch):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h = self.ec1(self.X)\n",
        "                z = self.dc4(h)\n",
        "                loss = tf.reduce_mean(MSE(count, z))\n",
        "            vars = self.autoencoder1.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 10 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        return h\n",
        "        \n",
        "    def train2(self, h, max_epoch=120, lr=0.005):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr)\n",
        "        for epoch in range(0, max_epoch):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h1 = self.ec2(h)\n",
        "                z = self.dc3(h1)\n",
        "                loss = tf.reduce_mean(MSE(h, z))\n",
        "            vars = self.autoencoder2.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 50 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        return h1\n",
        "    \n",
        "    def train3(self, h, max_epoch=350, lr=0.01):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr)\n",
        "        for epoch in range(0, max_epoch):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h1 = self.ec3(h)\n",
        "                z = self.dc2(h1)\n",
        "                loss = tf.reduce_mean(MSE(h, z))\n",
        "\n",
        "            vars = self.autoencoder3.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 50 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        return h1\n",
        "        \n",
        "    def train4(self, h, max_epoch=350, lr=0.01):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr)\n",
        "        for epoch in range(0, max_epoch):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h1 = self.ec4(h)\n",
        "                z = self.dc1(h1)\n",
        "                loss = tf.reduce_mean(MSE(h, z))\n",
        "\n",
        "            vars = self.autoencoder4.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 50 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        return h1\n",
        "\n",
        "    def train(self, max_epoch=100, lr=0.001):\n",
        "        weight0 = self.AE.get_weights()\n",
        "        optimizer = tf.keras.optimizers.Adam(lr)\n",
        "        for epoch in range(1, max_epoch+1):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                \n",
        "                h = self.ec(self.X)\n",
        "                z = self.dc(h)\n",
        "\n",
        "                a = K.eval(tf.math.is_nan(h))\n",
        "                b = K.eval(tf.math.is_inf(h))\n",
        "                if a.any() or b.any():\n",
        "                    #print(self.AE.weights)\n",
        "                    self.AE.set_weights(weight0)\n",
        "                    #print(self.AE.weights)\n",
        "                    break;\n",
        "\n",
        "                z1 = self.dc1(h)\n",
        "                z1 = self.dc2(z1)\n",
        "                z1 = self.dc3(z1)\n",
        "                pi, disp, mean = self.nb(z1)\n",
        "                zinb_loss = cal_zinb(pi, disp, 0, z, x_train)\n",
        "\n",
        "                #loss = tf.reduce_mean(MSE(count, z)) + 0.30 * zinb_loss\n",
        "                loss = tf.reduce_mean(MSE(count, z))\n",
        "                weight0 = self.AE.get_weights()\n",
        "            \n",
        "            vars = self.AE.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 10 == 0:\n",
        "                print(epoch)\n",
        "                print(loss)\n",
        "                print(zinb_loss)\n",
        "        print(\"Finish!\")\n",
        "    \n",
        "    def clustering_train(self):\n",
        "        self.autoencoder1.trainable = False\n",
        "        self.autoencoder2.trainable = False\n",
        "        self.autoencoder3.trainable = False\n",
        "\n",
        "        k = 8\n",
        "        h = self.ec(self.X)\n",
        "        h1 = self.dc1(h)\n",
        "        centers, labels = get_centers(np.array(h))\n",
        "        labels1 = KMeans(n_clusters=k, random_state=0).fit(h1).labels_\n",
        "        labels2 = KMeans(n_clusters=k, random_state=0).fit(h).labels_\n",
        "        mat = make_matrix(labels1, labels2, k)\n",
        "        labels1 = update_labels(labels1, mat)\n",
        "        centers1 = computeCentroids(h1, labels1)\n",
        "        centers2 = computeCentroids(h, labels2)\n",
        "\n",
        "        print(h.shape)\n",
        "        print(h1.shape)\n",
        "\n",
        "        q = cal_cluster(h, centers, 1.0)\n",
        "        p = target_distribution(q)\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        for epoch in range(1, 51):\n",
        "            if epoch % 10 == 0:\n",
        "                centers, labels = get_centers(np.array(h))\n",
        "                labels1 = KMeans(n_clusters=k, random_state=0).fit(h1).labels_\n",
        "                labels2 = KMeans(n_clusters=k, random_state=0).fit(h).labels_\n",
        "                mat = make_matrix(labels1, labels2, k)\n",
        "                labels1 = update_labels(labels1, mat)\n",
        "                centers1 = computeCentroids(h1, labels1)\n",
        "                centers2 = computeCentroids(h, labels2)\n",
        "\n",
        "                print(labels1[0:10])\n",
        "                print(labels2[0:10])\n",
        "\n",
        "                q = cal_cluster(h, centers, 1.0)\n",
        "                p = target_distribution(q)\n",
        "                q1 = cal_cluster(h1, centers1, 1.0)\n",
        "                q2 = cal_cluster(h, centers2, 1.0)\n",
        "\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h = self.ec(self.X)\n",
        "                z = self.dc(h)\n",
        "                loss = tf.reduce_mean(MSE(count, z))\n",
        "                \n",
        "                # clustering loss\n",
        "                q_out = cal_cluster(h, centers, 1.0)\n",
        "                cluster_loss = tf.reduce_mean(KLD(q_out, p))\n",
        "                loss = loss + 2.5 * cluster_loss\n",
        "\n",
        "                # k-means center loss\n",
        "                latent_dist1, latent_dist2 = cal_dist(h, centers)\n",
        "                kmeans_loss = tf.reduce_mean(tf.reduce_sum(latent_dist2, axis=1))\n",
        "                loss = loss + 0.1 * kmeans_loss\n",
        "\n",
        "                # cell-cell KLD loss\n",
        "                num, latent_p = cal_latent(h, 1)\n",
        "                latent_q = target_dis(latent_p)\n",
        "                latent_p = latent_p + tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "                latent_q = latent_q + tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "                kl_loss = tf.reduce_mean(KLD(latent_p, latent_q))\n",
        "                loss = loss + 2.5 * kl_loss\n",
        "\n",
        "                # Consistency loss\n",
        "                h1 = self.dc1(h)\n",
        "                q1 = cal_cluster(h1, centers1, 1.0)\n",
        "                q2 = cal_cluster(h, centers2, 1.0)\n",
        "                if q2.shape == q1.shape:\n",
        "                    cluster_loss_1 = tf.reduce_mean(KLD(q2, q1))\n",
        "                else:\n",
        "                    cluster_loss_1 = 0\n",
        "                loss = loss + 2.5 * cluster_loss_1\n",
        "            vars = self.autoencoder.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 10 == 0:\n",
        "                print(loss)\n",
        "                print(cluster_loss)\n",
        "                print(kmeans_loss)\n",
        "                print(kl_loss)\n",
        "                print(cluster_loss_1)\n",
        "        print(\"Finish!\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5127165b",
      "metadata": {
        "id": "5127165b"
      },
      "outputs": [],
      "source": [
        "autoencoder = SAE(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917ca689",
      "metadata": {
        "id": "917ca689"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686c681e",
      "metadata": {
        "id": "686c681e"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train2(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c493c73d",
      "metadata": {
        "id": "c493c73d"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train3(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c95a8e",
      "metadata": {
        "id": "08c95a8e"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train4(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87417bd7",
      "metadata": {
        "id": "87417bd7"
      },
      "outputs": [],
      "source": [
        "autoencoder.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d071fa07",
      "metadata": {
        "id": "d071fa07"
      },
      "outputs": [],
      "source": [
        "autoencoder.clustering_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42dbac5",
      "metadata": {
        "id": "a42dbac5"
      },
      "outputs": [],
      "source": [
        "encoded_data = autoencoder.ec(x_train)\n",
        "ed = np.array(encoded_data)\n",
        "ed = dotsne(ed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myscatter(ed, idents, legend=True)"
      ],
      "metadata": {
        "id": "k1iU5wD9pAnu"
      },
      "id": "k1iU5wD9pAnu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8def1262",
      "metadata": {
        "id": "8def1262"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, homogeneity_score, completeness_score\n",
        "def measure(true, pred):\n",
        "    NMI = round(normalized_mutual_info_score(true, pred), 2)\n",
        "    RAND = round(adjusted_rand_score(true, pred), 2)\n",
        "    HOMO = round(homogeneity_score(true, pred), 2)\n",
        "    COMP = round(completeness_score(true, pred), 2)\n",
        "    return [NMI, RAND, HOMO, COMP]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels, _,  _ = phenograph.cluster(np.array(encoded_data))\n",
        "measure(idents, labels)"
      ],
      "metadata": {
        "id": "xgpDBpA0bGUz"
      },
      "id": "xgpDBpA0bGUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d1e5da",
      "metadata": {
        "id": "a4d1e5da"
      },
      "outputs": [],
      "source": [
        "myscatter(ed,labels, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = KMeans(n_clusters=7, random_state=0).fit(ed).labels_\n",
        "measure(idents, labels)"
      ],
      "metadata": {
        "id": "9vBGpsLnqpdh"
      },
      "id": "9vBGpsLnqpdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myscatter(ed,labels, legend=True)"
      ],
      "metadata": {
        "id": "BcLWDVbsqxAF"
      },
      "id": "BcLWDVbsqxAF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a33ff1",
      "metadata": {
        "id": "78a33ff1"
      },
      "outputs": [],
      "source": [
        "ed = dotsne(count)\n",
        "myscatter(ed, idents, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = decomposition.PCA(n_components = 2)\n",
        "pca.fit(count)\n",
        "count_pca = pca.transform(count)"
      ],
      "metadata": {
        "id": "6lKSRjfM9ZpL"
      },
      "id": "6lKSRjfM9ZpL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myscatter(count_pca, idents, legend=True)"
      ],
      "metadata": {
        "id": "AnF84GQz9dOG"
      },
      "id": "AnF84GQz9dOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = pd.DataFrame(labels)\n",
        "save.to_csv('/content/drive/My Drive/compbio/test_data/bm4_predict.csv',index=False,header=True)"
      ],
      "metadata": {
        "id": "sOgWyHpgsfi4"
      },
      "id": "sOgWyHpgsfi4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = pd.DataFrame(np.array(encoded_data))\n",
        "save.to_csv('/content/drive/My Drive/compbio/test_data/bm4_encoded.csv',index=False,header=True)"
      ],
      "metadata": {
        "id": "UEKJ8hBQsowM"
      },
      "id": "UEKJ8hBQsowM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = pd.DataFrame(ed)\n",
        "save.to_csv('/content/drive/My Drive/compbio/test_data/bm4_2d.csv',index=False,header=True)"
      ],
      "metadata": {
        "id": "vsbtxe4is0Lq"
      },
      "id": "vsbtxe4is0Lq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hungarian-algorithm"
      ],
      "metadata": {
        "id": "vmUSoIFY7CLQ"
      },
      "id": "vmUSoIFY7CLQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0,1,2,1,0,3,3,3,3,4,4])\n",
        "b = np.array([2,1,0,1,2,3,3,3,4,4,4])"
      ],
      "metadata": {
        "id": "tImT0lMF7wSQ"
      },
      "id": "tImT0lMF7wSQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchtext import data, datasets\n",
        " \n",
        "if torch.cuda.is_available:\n",
        "  print('PyTorch can use GPUs!')\n",
        "else:\n",
        "  print('PyTorch cannot use GPUs.') "
      ],
      "metadata": {
        "id": "sih5DTKPOpBy"
      },
      "id": "sih5DTKPOpBy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = x_train\n",
        "y_true = x_train\n",
        "eps = 1e-10\n",
        "scale_factor=1.0\n",
        "theta = 1e6\n",
        "t1 = tf.math.lgamma(theta+eps) + tf.math.lgamma(y_true+1.0) - tf.math.lgamma(y_true+theta+eps)\n",
        "t2 = (theta+y_true) * tf.math.log(1.0 + (y_pred/(theta+eps))) + (y_true * (tf.math.log(theta+eps) - tf.math.log(y_pred+eps)))\n",
        "final = t1 + t2\n",
        "final = _nan2inf(final)"
      ],
      "metadata": {
        "id": "ZVNetfYdHzFd"
      },
      "id": "ZVNetfYdHzFd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _nan2inf(x):\n",
        "    return tf.where(tf.math.is_nan(x), tf.zeros_like(x)+np.inf, x)"
      ],
      "metadata": {
        "id": "gYxmqBIZJCEW"
      },
      "id": "gYxmqBIZJCEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "id": "uhufVwA7I7Yy"
      },
      "id": "uhufVwA7I7Yy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = 0.1\n",
        "nb_case = final - tf.math.log(1.0-pi+eps)\n",
        "zero_nb = tf.math.pow(theta/(theta+y_pred+eps), theta)\n",
        "zero_case = -tf.math.log(pi + ((1.0-pi)*zero_nb)+eps)\n",
        "result = tf.where(tf.less(y_true, 1e-8), zero_case, nb_case)\n",
        "ridge = 0*tf.square(pi)\n",
        "result += ridge\n",
        "result = tf.reduce_mean(result)\n",
        "result = _nan2inf(result)"
      ],
      "metadata": {
        "id": "0FqerRGyJBlH"
      },
      "id": "0FqerRGyJBlH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_case"
      ],
      "metadata": {
        "id": "Vckcumm7Ijdu"
      },
      "id": "Vckcumm7Ijdu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy"
      ],
      "metadata": {
        "id": "tEFVv_dcKcuR"
      },
      "id": "tEFVv_dcKcuR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anndata as ad\n",
        "import scanpy as sc"
      ],
      "metadata": {
        "id": "xz42_1y8KfbR"
      },
      "id": "xz42_1y8KfbR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/idents3_raw_1.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/data3_raw_1.csv\", index_col=0, sep=\",\"))"
      ],
      "metadata": {
        "id": "5gqyMvaqKjA0"
      },
      "id": "5gqyMvaqKjA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/idents1_raw_1.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/data1_raw_1.csv\", index_col=0, sep=\",\"))"
      ],
      "metadata": {
        "id": "G9Y5mxrO4_-4"
      },
      "id": "G9Y5mxrO4_-4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/pbmc_idents.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/pbmc_raw.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "ZUIx5gjd6MmV"
      },
      "id": "ZUIx5gjd6MmV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/colon_cell_info.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/colon_raw.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "Y9Isbsd67dHc"
      },
      "id": "Y9Isbsd67dHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/ileum_cell_info.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/ileum_raw.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "DV0MkKnH8mZi"
      },
      "id": "DV0MkKnH8mZi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/bm4_idents.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/bm4_raw.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "ewKGcVZL-NYt"
      },
      "id": "ewKGcVZL-NYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/compbio/test_data/IPMN_raw.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "8RwXUKFfrRei"
      },
      "id": "8RwXUKFfrRei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.transpose(count)"
      ],
      "metadata": {
        "id": "HqZ1V-Ap_kTj"
      },
      "id": "HqZ1V-Ap_kTj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata=sc.AnnData(count)"
      ],
      "metadata": {
        "id": "LIZQ5w0WK4gX"
      },
      "id": "LIZQ5w0WK4gX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata"
      ],
      "metadata": {
        "id": "qEG8TYMnAb9L"
      },
      "id": "qEG8TYMnAb9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.pp.normalize_per_cell(adata)"
      ],
      "metadata": {
        "id": "WFgNTA2SLQnY"
      },
      "id": "WFgNTA2SLQnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = adata.X"
      ],
      "metadata": {
        "id": "f16fkC0ULasx"
      },
      "id": "f16fkC0ULasx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count.shape"
      ],
      "metadata": {
        "id": "_D1suSo7Akgw"
      },
      "id": "_D1suSo7Akgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize, scale"
      ],
      "metadata": {
        "id": "Wzt8taLoQfLR"
      },
      "id": "Wzt8taLoQfLR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_norm = scale(count)\n",
        "count = np.transpose(count_norm)"
      ],
      "metadata": {
        "id": "kLgbIexrT-Mr"
      },
      "id": "kLgbIexrT-Mr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_norm.shape"
      ],
      "metadata": {
        "id": "qU_KZrsMUaP8"
      },
      "id": "qU_KZrsMUaP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.var(count, axis=0)"
      ],
      "metadata": {
        "id": "MZbOudjCRDAl"
      },
      "id": "MZbOudjCRDAl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.var(normalize(count), axis=1)"
      ],
      "metadata": {
        "id": "pv0NDHOyWOuo"
      },
      "id": "pv0NDHOyWOuo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.var(normalize(count), axis=0)"
      ],
      "metadata": {
        "id": "hXIlv4uUWVtr"
      },
      "id": "hXIlv4uUWVtr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.var(normalize(np.transpose(count)), axis=1)"
      ],
      "metadata": {
        "id": "6Qkd3dvPWX6N"
      },
      "id": "6Qkd3dvPWX6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.var(normalize(np.transpose(count)), axis=0)"
      ],
      "metadata": {
        "id": "il-gb3eJWiBx"
      },
      "id": "il-gb3eJWiBx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.transpose(count)"
      ],
      "metadata": {
        "id": "5tqukOPJX38J"
      },
      "id": "5tqukOPJX38J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(count, axis=0)"
      ],
      "metadata": {
        "id": "2A_D8aESiXWQ"
      },
      "id": "2A_D8aESiXWQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy louvain\n",
        "!pip install git+https://github.com/saketkc/pysctransform.git"
      ],
      "metadata": {
        "id": "iNu4xJP9wr5M"
      },
      "id": "iNu4xJP9wr5M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pysctransform import vst, get_hvg_residuals, SCTransform\n",
        "import anndata as ad\n",
        "import scanpy as sc\n",
        "from scipy.sparse import *"
      ],
      "metadata": {
        "id": "aC9uLI9vxlXD"
      },
      "id": "aC9uLI9vxlXD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colon, ileum, bm4 no transpose\n",
        "# pbmc, sim transpose"
      ],
      "metadata": {
        "id": "bg2DKcZg7oqd"
      },
      "id": "bg2DKcZg7oqd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata=sc.AnnData(count)\n",
        "adata.X = csr_matrix(adata.X)\n",
        "residuals = SCTransform(adata, var_features_n=3000)"
      ],
      "metadata": {
        "id": "o-SBWs_0xS5m"
      },
      "id": "o-SBWs_0xS5m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(residuals)"
      ],
      "metadata": {
        "id": "6KGoPxpp23nw"
      },
      "id": "6KGoPxpp23nw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/compbio/test_data/\")"
      ],
      "metadata": {
        "id": "rA0AKoI5WtkT"
      },
      "id": "rA0AKoI5WtkT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('ileum_norm.npy',count)"
      ],
      "metadata": {
        "id": "Bi9tvzcBZdUy"
      },
      "id": "Bi9tvzcBZdUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('IPMN_pysct.npy',count)"
      ],
      "metadata": {
        "id": "EFTZuKguXLXu"
      },
      "id": "EFTZuKguXLXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('ileum_idents.npy',idents)"
      ],
      "metadata": {
        "id": "MA9lSvvvXjhZ"
      },
      "id": "MA9lSvvvXjhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = residuals.columns.values\n",
        "res = res.astype('int32')\n",
        "res.sort()\n",
        "count = np.transpose(count)\n",
        "count0 = count[:,res]\n",
        "adata = ad.AnnData(np.array(residuals), dtype='float32')\n",
        "adata.layers[\"counts\"] = count0\n",
        "adata.raw = adata"
      ],
      "metadata": {
        "id": "n2-_3vP3f2Yv"
      },
      "id": "n2-_3vP3f2Yv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata = ad.AnnData(count, dtype='int32')\n",
        "adata.layers[\"counts\"] = adata.X.copy()\n",
        "sc.pp.normalize_total(adata, target_sum=1e4)\n",
        "sc.pp.log1p(adata)\n",
        "adata.raw = adata"
      ],
      "metadata": {
        "id": "2H-qVLe7_q6f"
      },
      "id": "2H-qVLe7_q6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata.X"
      ],
      "metadata": {
        "id": "VC41PZWS_5uo"
      },
      "id": "VC41PZWS_5uo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}