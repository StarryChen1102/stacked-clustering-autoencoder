{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9bf651",
      "metadata": {
        "id": "4f9bf651"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import sklearn\n",
        "import tensorflow as tf\n",
        "#import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from tensorflow.keras.losses import MSE, KLD\n",
        "import phenograph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phenograph"
      ],
      "metadata": {
        "id": "r2Z4YzQXlMeb"
      },
      "id": "r2Z4YzQXlMeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V6f39ED9nmNf"
      },
      "id": "V6f39ED9nmNf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fdeefb",
      "metadata": {
        "id": "70fdeefb"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Dense\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "#from keras.datasets import mnist\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac04ef15",
      "metadata": {
        "id": "ac04ef15"
      },
      "outputs": [],
      "source": [
        "def myscatter(Y, class_idxs, legend=False, ran=True, seed=229):\n",
        "    if ran:\n",
        "        np.random.seed(seed)\n",
        "    Y = np.array(Y)\n",
        "    fig, ax = plt.subplots(figsize=(6,4), dpi=300)\n",
        "    classes = list(np.unique(class_idxs))\n",
        "    markers = 'osD' * len(classes)\n",
        "    colors = plt.cm.rainbow(np.linspace(0, 1, len(classes)))\n",
        "    if ran:\n",
        "        np.random.shuffle(colors)\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        mark = markers[i]\n",
        "        ax.plot(Y[class_idxs == cls, 0], Y[class_idxs == cls, 1], marker=mark,\n",
        "                linestyle='', ms=4, label=str(cls), alpha=1, color=colors[i],\n",
        "                markeredgecolor='black', markeredgewidth=0.15)\n",
        "    if legend:\n",
        "        ax.legend(bbox_to_anchor=(1.03, 1), loc=2, borderaxespad=0, fontsize=10, markerscale=2, frameon=False,\n",
        "                  ncol=2, handletextpad=0.1, columnspacing=0.5)\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba07b5d",
      "metadata": {
        "id": "5ba07b5d"
      },
      "outputs": [],
      "source": [
        "def dotsne(X, dim=2, ran=23):\n",
        "    tsne = TSNE(n_components=dim, random_state=ran)\n",
        "    Y_tsne = tsne.fit_transform(X)\n",
        "    return Y_tsne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd50bbdd",
      "metadata": {
        "id": "bd50bbdd"
      },
      "outputs": [],
      "source": [
        "count = np.load('/content/drive/My Drive/CompBio/test_data/count.npy')\n",
        "idents = np.load('/content/drive/My Drive/CompBio/test_data/idents.npy')\n",
        "#count = np.array(pd.read_csv(\"real_raw.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.transpose(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78aa1ee",
      "metadata": {
        "id": "b78aa1ee"
      },
      "outputs": [],
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/idents1.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/data1.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e795b557",
      "metadata": {
        "id": "e795b557"
      },
      "outputs": [],
      "source": [
        "idents = np.load('idents.npy')\n",
        "count = np.array(pd.read_csv(\"pbmc_raw.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d417ea6",
      "metadata": {
        "id": "7d417ea6"
      },
      "outputs": [],
      "source": [
        "count = np.array(pd.read_csv(\"demo_counts.csv\", index_col=0, sep=\"\\t\"))\n",
        "idents = np.load('idents.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095d3636",
      "metadata": {
        "id": "095d3636"
      },
      "outputs": [],
      "source": [
        "idents = np.array(pd.read_csv(\"ileum_cell_info.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"ileum_raw.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8303f542",
      "metadata": {
        "id": "8303f542"
      },
      "outputs": [],
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/ileum_cell_info_SCT.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/ileum_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0f4825",
      "metadata": {
        "id": "1d0f4825"
      },
      "outputs": [],
      "source": [
        "idents = np.array(pd.read_csv(\"colon_cell_info.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"colon_raw.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09512f79",
      "metadata": {
        "id": "09512f79"
      },
      "outputs": [],
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/colon_cell_info_SCT.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/colon_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ade723",
      "metadata": {
        "id": "a3ade723"
      },
      "outputs": [],
      "source": [
        "idents = np.load('idents.npy')\n",
        "count = np.array(pd.read_csv(\"breast_cancer.csv\", index_col=0, sep=\"\\t\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28af3e24",
      "metadata": {
        "id": "28af3e24"
      },
      "outputs": [],
      "source": [
        "count = np.array(pd.read_csv(\"drug_response_normalize.csv\", index_col=0, sep=\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/BM4_select_SCT_labels.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/BM4_select_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "VDGRQQ6okdNz"
      },
      "id": "VDGRQQ6okdNz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/GSM3587923_AML1012-D0_SCT_labels.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/GSM3587923_AML1012-D0_SCT.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "NAVHYcaBn4N0"
      },
      "id": "NAVHYcaBn4N0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/AML707B-all_celltype_select.csv\", index_col=0, sep=\"\\t\"))\n",
        "#cells_number = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/GSM3587969_AML707B-D0_info_number.csv\", index_col=0, sep=\"\\t\"))\n",
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/AML707B-all_normalize_select.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "3zxPAwNUfOf3"
      },
      "id": "3zxPAwNUfOf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/pbmc_SCT.csv\", index_col=0, sep=\"\\t\"))\n",
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/pbmc_idents.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "js7E2zibyw20"
      },
      "id": "js7E2zibyw20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/real_data.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "fcJGZA_BIKHH"
      },
      "id": "fcJGZA_BIKHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/normal_pancreas.csv\", index_col=0, sep=\"\\t\"))\n",
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/idents_normal_pancreas.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "MV3TvVgqKhLc"
      },
      "id": "MV3TvVgqKhLc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/IPMN_data.csv\", index_col=0, sep=\"\\t\"))\n",
        "idents = np.array(pd.read_csv(\"/content/drive/My Drive/CompBio/test_data/idents_IPMN.csv\", index_col=0, sep=\"\\t\"))"
      ],
      "metadata": {
        "id": "CnNYuFkkRJ_l"
      },
      "id": "CnNYuFkkRJ_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import decomposition"
      ],
      "metadata": {
        "id": "g9-vKAThwyAY"
      },
      "id": "g9-vKAThwyAY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988e058d",
      "metadata": {
        "id": "988e058d"
      },
      "outputs": [],
      "source": [
        "idents = idents.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c2def9",
      "metadata": {
        "id": "f8c2def9"
      },
      "outputs": [],
      "source": [
        "[n_sample, n_gene] = count.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8796008e",
      "metadata": {
        "id": "8796008e"
      },
      "outputs": [],
      "source": [
        "x_train = count.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idents = idents.astype('str')"
      ],
      "metadata": {
        "id": "gtIOayX8gAyS"
      },
      "id": "gtIOayX8gAyS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9136fdb",
      "metadata": {
        "id": "d9136fdb"
      },
      "outputs": [],
      "source": [
        "idents = np.zeros(n_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3e3dae",
      "metadata": {
        "id": "1a3e3dae"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import SpectralClustering\n",
        "def clustering(h, n_cluster, k=15, f=\"kmeans\"):\n",
        "    if f == \"kmeans\":\n",
        "        labels = KMeans(n_clusters=n_cluster, random_state=0).fit(h).labels_\n",
        "    elif f == \"spectral\":\n",
        "        labels = SpectralClustering(n_clusters=n_cluster, affinity=\"precomputed\", assign_labels=\"discretize\",\n",
        "                                    random_state=0).fit_predict(adj)\n",
        "    \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433cce48",
      "metadata": {
        "id": "433cce48"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import kneighbors_graph\n",
        "def find_neighbors(X):\n",
        "    A = kneighbors_graph(X, 10, mode=\"connectivity\", metric=\"euclidean\", include_self=True)\n",
        "    return A.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc809b2c",
      "metadata": {
        "id": "cc809b2c"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "class DenseTranspose(keras.layers.Layer):\n",
        "    def __init__(self, dense, activation=None, **kwargs):\n",
        "        self.dense = dense\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        super().__init__(**kwargs)\n",
        "    def build(self, batch_input_shape):\n",
        "        self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\",\n",
        "                                      shape=[self.dense.input_shape[-1]])\n",
        "        super().build(batch_input_shape)\n",
        "    def call(self, inputs):\n",
        "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
        "        return self.activation(z + self.biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeda504c",
      "metadata": {
        "id": "aeda504c"
      },
      "outputs": [],
      "source": [
        "encoded1 = Dense(512, activation = \"relu\", kernel_constraint = keras.constraints.NonNeg(), input_shape=(n_gene,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "\n",
        "encoded2 = Dense(256, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(512,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "\n",
        "encoded3 = Dense(128, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(256,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "\n",
        "encoded4 = Dense(25, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(128,), use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',bias_initializer='zeros')\n",
        "\n",
        "encoder = keras.models.Sequential([encoded1, encoded2, encoded3, encoded4])\n",
        "encoder1 = keras.models.Sequential([encoded1])\n",
        "encoder2 = keras.models.Sequential([encoded2])\n",
        "encoder3 = keras.models.Sequential([encoded3])\n",
        "encoder4 = keras.models.Sequential([encoded4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85df5dff",
      "metadata": {
        "id": "85df5dff"
      },
      "outputs": [],
      "source": [
        "decoded1 = DenseTranspose(encoded4, activation = \"relu\")\n",
        "\n",
        "decoded2 = DenseTranspose(encoded3, activation = \"relu\")\n",
        "\n",
        "decoded3 = DenseTranspose(encoded2, activation = \"relu\")\n",
        "\n",
        "decoded4 = DenseTranspose(encoded1, activation = \"relu\")\n",
        "\n",
        "decoder = keras.models.Sequential([decoded1, decoded2, decoded3, decoded4])\n",
        "decoder1 = keras.models.Sequential([decoded1])\n",
        "decoder2 = keras.models.Sequential([decoded2])\n",
        "decoder3 = keras.models.Sequential([decoded3])\n",
        "decoder4 = keras.models.Sequential([decoded4])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no\n",
        "decoded1 = Dense(128, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(25,), use_bias=True)\n",
        "\n",
        "decoded2 = Dense(256, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(128,), use_bias=True)\n",
        "\n",
        "decoded3 = Dense(512, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(256,), use_bias=True)\n",
        "\n",
        "decoded4 = Dense(n_gene, activation = \"relu\", activity_regularizer = regularizers.l1(10e-4), input_shape=(512,), use_bias=True)\n",
        "\n",
        "decoder = keras.models.Sequential([decoded1, decoded2, decoded3, decoded4])\n",
        "decoder1 = keras.models.Sequential([decoded1])\n",
        "decoder2 = keras.models.Sequential([decoded2])\n",
        "decoder3 = keras.models.Sequential([decoded3])\n",
        "decoder4 = keras.models.Sequential([decoded4])"
      ],
      "metadata": {
        "id": "0deKzMjrto2X"
      },
      "id": "0deKzMjrto2X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae02c77",
      "metadata": {
        "id": "eae02c77"
      },
      "outputs": [],
      "source": [
        "autoencoder1 = keras.models.Sequential([encoded1, decoded4])\n",
        "autoencoder2 = keras.models.Sequential([encoded2, decoded3])\n",
        "autoencoder3 = keras.models.Sequential([encoded3, decoded2])\n",
        "autoencoder4 = keras.models.Sequential([encoded4, decoded1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no\n",
        "h = autoencoder.ec(x_train)\n",
        "h1 = autoencoder.ec1(x_train)\n",
        "h1 = autoencoder.ec2(h1)\n",
        "centers, labels = get_centers(np.array(h))\n",
        "centers1, labels1 = get_centers(np.array(h1))\n",
        "q = cal_cluster(h, centers, 1.0)\n",
        "q1 = cal_cluster(h1, centers1, 1.0)\n",
        "tf.reduce_mean(KLD(q, q1))"
      ],
      "metadata": {
        "id": "UHxKbFclzA-a"
      },
      "id": "UHxKbFclzA-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, _,  _ = phenograph.cluster(np.array(h))\n",
        "print(labels[0:50])\n",
        "labels, _,  _ = phenograph.cluster(np.array(h1))\n",
        "print(labels[0:50])"
      ],
      "metadata": {
        "id": "CXdjZ9YHCxep"
      },
      "id": "CXdjZ9YHCxep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f414b22b",
      "metadata": {
        "id": "f414b22b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import activations, constraints, initializers, regularizers\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class ClusteringLayer(Layer):\n",
        "    def __init__(self, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=None, initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "def target_distribution(q):\n",
        "    q = q.numpy()\n",
        "    weight = q ** 2 / q.sum(0)\n",
        "    return (weight.T / weight.sum(1)).T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_cluster(hidden, clusters, alpha):\n",
        "    clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
        "    q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(hidden, axis=1) - clusters), axis=2) / alpha))\n",
        "    q **= (alpha + 1.0) / 2.0\n",
        "    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "    return q"
      ],
      "metadata": {
        "id": "FMLYPrl587D7"
      },
      "id": "FMLYPrl587D7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_dist(hidden, clusters):\n",
        "    clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
        "    dist1 = K.sum(K.square(K.expand_dims(hidden, axis=1) - clusters), axis=2)\n",
        "    temp_dist1 = dist1 - tf.reshape(tf.reduce_min(dist1, axis=1), [-1, 1])\n",
        "    temp_dist1 = K.transpose(K.transpose(temp_dist1) / K.max(temp_dist1, axis=1))\n",
        "    q = K.exp(-temp_dist1 * 10)\n",
        "    #q = K.exp(-temp_dist1)\n",
        "    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "    q = K.pow(q, 2)\n",
        "    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "    dist2 = dist1 * q\n",
        "    return dist1, dist2"
      ],
      "metadata": {
        "id": "pyBiZrOfYCkM"
      },
      "id": "pyBiZrOfYCkM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_latent(hidden, alpha):\n",
        "    sum_y = K.sum(K.square(hidden), axis=1)\n",
        "    num = -2.0 * tf.matmul(hidden, tf.transpose(hidden)) + tf.reshape(sum_y, [-1, 1]) + sum_y\n",
        "    num = num / alpha\n",
        "    num = tf.pow(1.0 + num, -(alpha + 1.0) / 2.0)\n",
        "    zerodiag_num = num - tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "    latent_p = K.transpose(K.transpose(zerodiag_num) / K.sum(zerodiag_num, axis=1))\n",
        "    return num, latent_p\n",
        "\n",
        "def target_dis(latent_p):\n",
        "    latent_q = tf.transpose(tf.transpose(tf.pow(latent_p, 2)) / tf.reduce_sum(latent_p, axis = 1))\n",
        "    return tf.transpose(tf.transpose(latent_q) / tf.reduce_sum(latent_q, axis = 1))"
      ],
      "metadata": {
        "id": "rwc_9jScUgmL"
      },
      "id": "rwc_9jScUgmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2a870a",
      "metadata": {
        "id": "4d2a870a"
      },
      "outputs": [],
      "source": [
        "def computeCentroids(data, labels):\n",
        "    n_clusters = len(np.unique(labels))\n",
        "    return np.array([data[labels == i].mean(0) for i in range(n_clusters)])\n",
        "\n",
        "def get_centers(Y):\n",
        "    #from sklearn.cluster import SpectralClustering\n",
        "    l, _,  _ = phenograph.cluster(Y)\n",
        "    #l = KMeans(n_clusters=10, random_state=0).fit(Y).labels_\n",
        "    centers = computeCentroids(Y, l)\n",
        "    return centers, l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721433ee",
      "metadata": {
        "id": "721433ee"
      },
      "outputs": [],
      "source": [
        "clustering_layer = ClusteringLayer(name='clustering', input_shape=(25,))\n",
        "clustering = keras.models.Sequential([clustering_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27637baf",
      "metadata": {
        "id": "27637baf"
      },
      "outputs": [],
      "source": [
        "class AE(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, encoder, decoder, clustering):\n",
        "        super(AE, self).__init__()\n",
        "        self.ec = encoder\n",
        "        self.dc = decoder\n",
        "        self.clustering = clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d838ea",
      "metadata": {
        "id": "74d838ea"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import MSE, KLD\n",
        "import math\n",
        "\n",
        "class SAE():\n",
        "\n",
        "    def __init__(self, X):\n",
        "        super(SAE, self).__init__()\n",
        "        self.X = X\n",
        "        self.ec1 = encoder1\n",
        "        self.ec2 = encoder2\n",
        "        self.ec3 = encoder3\n",
        "        self.ec4 = encoder4\n",
        "        self.dc1 = decoder1\n",
        "        self.dc2 = decoder2\n",
        "        self.dc3 = decoder3\n",
        "        self.dc4 = decoder4\n",
        "        self.autoencoder1 = autoencoder1\n",
        "        self.autoencoder2 = autoencoder2\n",
        "        self.autoencoder3 = autoencoder3\n",
        "        self.autoencoder4 = autoencoder4\n",
        "        self.ec = encoder\n",
        "        self.dc = decoder\n",
        "        self.autoencoder = keras.models.Sequential([encoder, decoder])\n",
        "        self.clustering = clustering\n",
        "        self.AE = AE(encoder, decoder, clustering)\n",
        "        \n",
        "    def train1(self):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=0.002)\n",
        "        for epoch in range(0, 100):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h = self.ec1(self.X)\n",
        "                z = self.dc4(h)\n",
        "                #im = self.im(z)\n",
        "                \n",
        "                #impute_out = tf.multiply(self.I, im)\n",
        "                #impute_out = preprocessing.scale(impute_out)\n",
        "                \n",
        "                \n",
        "                #count1 = tf.multiply(self.I, self.X)\n",
        "                #z1 = tf.multiply(self.I, z)\n",
        "                \n",
        "                loss = tf.reduce_mean(MSE(count, z))\n",
        "                \n",
        "                #if epoch % 5 == 0:\n",
        "                    #A = np.nonzero(find_neighbors(h))\n",
        "                    #Ax = A[0]\n",
        "                    #Ay = A[1]\n",
        "                    #A = np.zeros([n_sample, n_sample])\n",
        "                    #A[Ax, Ay] = 1\n",
        "                    #score = measure_matrix(A0, A, n_sample)\n",
        "                    #loss = loss + math.exp(0-score) / 2\n",
        "\n",
        "            vars = self.autoencoder1.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 10 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        #print(self.ec1.weights)\n",
        "        #print(self.dc4.weights)\n",
        "        #print(self.autoencoder1.weights)\n",
        "        return h\n",
        "        \n",
        "    def train2(self, h):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=0.005)\n",
        "        c = h\n",
        "        #c = preprocessing.scale(np.array(h))\n",
        "        for epoch in range(0, 120):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h1 = self.ec2(h)\n",
        "                z = self.dc3(h1)\n",
        "                loss = tf.reduce_mean(MSE(c, z))\n",
        "                \n",
        "\n",
        "            vars = self.autoencoder2.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 50 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        #print(self.ec1.weights)\n",
        "        return h1\n",
        "    \n",
        "    def train3(self, h):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
        "        c = h\n",
        "        #c = preprocessing.scale(np.array(h))\n",
        "        for epoch in range(0, 350):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h1 = self.ec3(h)\n",
        "                z = self.dc2(h1)\n",
        "                loss = tf.reduce_mean(MSE(c, z))\n",
        "\n",
        "            vars = self.autoencoder3.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 50 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        return h1\n",
        "        \n",
        "    def train4(self, h):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
        "        c = h\n",
        "        #c = preprocessing.scale(np.array(h))\n",
        "        for epoch in range(0, 350):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h1 = self.ec4(h)\n",
        "                z = self.dc1(h1)\n",
        "                loss = tf.reduce_mean(MSE(c, z))\n",
        "\n",
        "            vars = self.autoencoder4.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 50 == 0:\n",
        "                print(loss)\n",
        "        print(\"Finish!\")\n",
        "        return h1\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        for epoch in range(0, 100):\n",
        "            \n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h = self.ec(self.X)\n",
        "                z = self.dc(h)\n",
        "                loss = tf.reduce_mean(MSE(count, z))\n",
        "                \n",
        "            vars = self.AE.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 10 == 0:\n",
        "                print(loss)\n",
        "                #print(p)\n",
        "                #print(q_out)\n",
        "        print(\"Finish!\")\n",
        "    \n",
        "    def clustering_train(self):\n",
        "        #print(self.ec1.weights)\n",
        "        h = self.ec(self.X)\n",
        "        h1 = self.ec1(self.X)\n",
        "        h1 = self.ec2(h1)\n",
        "        centers, labels = get_centers(np.array(h))\n",
        "        centers1, labels1 = get_centers(np.array(h1))\n",
        "\n",
        "        print(h.shape)\n",
        "        print(h1.shape)\n",
        "        \n",
        "        #self.clustering.get_layer(name='clustering').clusters = centers\n",
        "        #q = self.clustering(h)\n",
        "        #p = target_distribution(q)\n",
        "\n",
        "        q = cal_cluster(h, centers, 1.0)\n",
        "        p = target_distribution(q)\n",
        "        q1 = cal_cluster(h1, centers1, 1.0)\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        for epoch in range(1, 51):\n",
        "            if epoch % 10 == 0:\n",
        "                centers, labels = get_centers(np.array(h))\n",
        "                centers1, labels1 = get_centers(np.array(h1))\n",
        "                #self.clustering.get_layer(name='clustering').clusters = centers\n",
        "                #q = self.clustering(h)\n",
        "                q = cal_cluster(h, centers, 1.0)\n",
        "                p = target_distribution(q)\n",
        "                q1 = cal_cluster(h1, centers1, 1.0)\n",
        "                #print(clustering_layer.clusters)\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                h = self.ec(self.X)\n",
        "                z = self.dc(h)\n",
        "                loss = tf.reduce_mean(MSE(count, z))\n",
        "                \n",
        "                #q_out = self.clustering(h)\n",
        "                q_out = cal_cluster(h, centers, 1.0)\n",
        "                cluster_loss = tf.reduce_mean(KLD(q_out, p))\n",
        "                loss = loss + 2.5 * cluster_loss\n",
        "\n",
        "                #centers, labels = get_centers(np.array(h))\n",
        "                latent_dist1, latent_dist2 = cal_dist(h, centers)\n",
        "                kmeans_loss = tf.reduce_mean(tf.reduce_sum(latent_dist2, axis=1))\n",
        "                loss = loss + 0.1 * kmeans_loss\n",
        "\n",
        "                num, latent_p = cal_latent(h, 1)\n",
        "                latent_q = target_dis(latent_p)\n",
        "                latent_p = latent_p + tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "                latent_q = latent_q + tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "                #cross_entropy = -tf.reduce_sum(latent_q * tf.math.log(latent_p))\n",
        "                #entropy = -tf.reduce_sum(latent_q * tf.math.log(latent_q))\n",
        "                #kl_loss = cross_entropy - entropy\n",
        "                kl_loss = tf.reduce_mean(KLD(latent_p, latent_q))\n",
        "                loss = loss + 2.5 * kl_loss\n",
        "\n",
        "                h1 = self.ec1(self.X)\n",
        "                h1 = self.ec2(h1)\n",
        "                q1 = cal_cluster(h1, centers1, 1.0)\n",
        "                if q.shape == q1.shape:\n",
        "                    cluster_loss_1 = tf.reduce_mean(KLD(q_out, q1))\n",
        "                    loss = loss + 2.5 * cluster_loss_1\n",
        "                else:\n",
        "                    cluster_loss_1 = 0\n",
        "\n",
        "\n",
        "            vars = self.autoencoder4.trainable_weights\n",
        "            #vars = self.AE.trainable_weights\n",
        "            grads = tape.gradient(loss, vars)\n",
        "            optimizer.apply_gradients(zip(grads, vars))\n",
        "            if epoch % 10 == 0:\n",
        "                print(loss)\n",
        "                print(cluster_loss)\n",
        "                print(kmeans_loss)\n",
        "                print(kl_loss)\n",
        "                print(cluster_loss_1)\n",
        "                #print(latent_dist2)\n",
        "                #print(latent_dist2.shape)\n",
        "                #print(clustering_layer.clusters)\n",
        "                #print(clustering_layer.weights)\n",
        "                #print(p)\n",
        "                #print(q_out)\n",
        "        print(\"Finish!\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5127165b",
      "metadata": {
        "id": "5127165b"
      },
      "outputs": [],
      "source": [
        "autoencoder = SAE(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917ca689",
      "metadata": {
        "id": "917ca689"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686c681e",
      "metadata": {
        "id": "686c681e"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train2(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c493c73d",
      "metadata": {
        "id": "c493c73d"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train3(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c95a8e",
      "metadata": {
        "id": "08c95a8e"
      },
      "outputs": [],
      "source": [
        "h = autoencoder.train4(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87417bd7",
      "metadata": {
        "id": "87417bd7"
      },
      "outputs": [],
      "source": [
        "autoencoder.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d071fa07",
      "metadata": {
        "id": "d071fa07"
      },
      "outputs": [],
      "source": [
        "autoencoder.clustering_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42dbac5",
      "metadata": {
        "id": "a42dbac5"
      },
      "outputs": [],
      "source": [
        "encoded_data = autoencoder.ec(x_train)\n",
        "ed = np.array(encoded_data)\n",
        "ed = dotsne(ed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myscatter(ed, idents, legend=True)"
      ],
      "metadata": {
        "id": "k1iU5wD9pAnu"
      },
      "id": "k1iU5wD9pAnu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#centers, labels = get_centers(np.array(encoded_data))\n",
        "dist1 = K.sum(K.square(K.expand_dims(encoded_data, axis=1) - centers), axis=2)\n",
        "temp_dist1 = dist1 - tf.reshape(tf.reduce_min(dist1, axis=1), [-1, 1])\n",
        "temp_dist1 = K.transpose(K.transpose(temp_dist1) / K.max(temp_dist1, axis=1))\n",
        "q = K.exp(-temp_dist1 * 10)\n",
        "q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "q = K.pow(q, 2)\n",
        "q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "dist2 = dist1 * q\n",
        "kmeans_loss = tf.reduce_mean(tf.reduce_sum(dist2, axis=1))\n",
        "kmeans_loss"
      ],
      "metadata": {
        "id": "oTVNRdELhN9A"
      },
      "id": "oTVNRdELhN9A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(encoded_data, axis=1) - centers), axis=2) / 1))\n",
        "q **= (1 + 1.0) / 2.0\n",
        "q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "q[1]"
      ],
      "metadata": {
        "id": "9IspWo7f7j32"
      },
      "id": "9IspWo7f7j32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num, latent_p = cal_latent(encoded_data, 1)\n",
        "latent_p = latent_p + tf.linalg.diag(tf.linalg.diag_part(num))\n",
        "latent_p[0:100,8]"
      ],
      "metadata": {
        "id": "JqPEkGWiaoQh"
      },
      "id": "JqPEkGWiaoQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8def1262",
      "metadata": {
        "id": "8def1262"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, homogeneity_score, completeness_score\n",
        "def measure(true, pred):\n",
        "    NMI = round(normalized_mutual_info_score(true, pred), 2)\n",
        "    RAND = round(adjusted_rand_score(true, pred), 2)\n",
        "    HOMO = round(homogeneity_score(true, pred), 2)\n",
        "    COMP = round(completeness_score(true, pred), 2)\n",
        "    return [NMI, RAND, HOMO, COMP]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels, _,  _ = phenograph.cluster(np.array(encoded_data))\n",
        "measure(idents, labels)"
      ],
      "metadata": {
        "id": "xgpDBpA0bGUz"
      },
      "id": "xgpDBpA0bGUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d1e5da",
      "metadata": {
        "id": "a4d1e5da"
      },
      "outputs": [],
      "source": [
        "myscatter(ed,labels, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = KMeans(n_clusters=12, random_state=0).fit(ed).labels_\n",
        "measure(idents, labels)"
      ],
      "metadata": {
        "id": "9vBGpsLnqpdh"
      },
      "id": "9vBGpsLnqpdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myscatter(ed,labels, legend=True)"
      ],
      "metadata": {
        "id": "BcLWDVbsqxAF"
      },
      "id": "BcLWDVbsqxAF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a33ff1",
      "metadata": {
        "id": "78a33ff1"
      },
      "outputs": [],
      "source": [
        "ed = dotsne(count)\n",
        "myscatter(ed, idents, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = decomposition.PCA(n_components = 2)\n",
        "pca.fit(count)\n",
        "count_pca = pca.transform(count)"
      ],
      "metadata": {
        "id": "6lKSRjfM9ZpL"
      },
      "id": "6lKSRjfM9ZpL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myscatter(count_pca, idents, legend=True)"
      ],
      "metadata": {
        "id": "AnF84GQz9dOG"
      },
      "id": "AnF84GQz9dOG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}